{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce773622-6554-471a-ba34-8135e9bc091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "import pandas as pd\n",
    "from winterdrb.utils import load_real_and_bogus, load_extra_background\n",
    "from winterdrb.paths import BASE_DATA_DIR, train_path\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from winterdrb.plot import generate_single_page\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import logging\n",
    "\n",
    "import warnings \n",
    "import glob, os\n",
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import glob, os\n",
    "import fastavro\n",
    "from fastavro import reader, writer\n",
    "import io\n",
    "import gzip\n",
    "from mirar.data.utils.compress import decode_img\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import ndimage\n",
    "import sqlite3\n",
    "import joblib\n",
    "\n",
    "from winterrb.utils import pick_alert, make_triplet, rotate_triplet, plot_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555e3240-b6ee-4f91-9d6c-73b452c32f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eadb767-6253-4204-923e-43a9fca24015",
   "metadata": {},
   "outputs": [],
   "source": [
    "redo_train_data = False\n",
    "\n",
    "if redo_train_data:\n",
    "    real_df, bogus_df = load_real_and_bogus()\n",
    "    real_df[\"known\"], bogus_df[\"known\"] = True, True\n",
    "    extra_bogus = load_extra_background(n_images=None)\n",
    "    extra_bogus[\"known\"] = False\n",
    "    bogus_df = pd.concat([bogus_df, extra_bogus], axis=0)\n",
    "    real_df[\"class\"], bogus_df[\"class\"] = True, False\n",
    "    train_df = pd.concat([real_df, bogus_df], axis=0).reset_index(drop=True)\n",
    "    train_df[\"ztfname\"] = train_df[\"ztfname\"].replace({np.nan: None, \"nan\": None})\n",
    "    train_df.to_parquet(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba12a1-7f49-4ffd-b690-05dc63b68820",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dea7f719-af1d-490f-8124-3c2d54b9cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "REALS = train_df[train_df['class']]\n",
    "BOGUS = train_df[~train_df['class']].sample(400)\n",
    "BOGUS.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01af0778-f8d2-4039-8d4d-2797805fdfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triplet(row: pd.Series, normalize: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Make stacked triplet from alert packet\n",
    "        \n",
    "    Parameters:\n",
    "    ----------\n",
    "    alert : dict\n",
    "        Alert packet\n",
    "    normalize: bool\n",
    "        Normalize the cutout counts; False by default\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    triplet: np.ndarray\n",
    "        Stacked triplet of science, reference and difference cutouts\n",
    "    \"\"\"\n",
    "    cutout_dict = dict()\n",
    "    \n",
    "    for cutout in ['science', 'template', 'difference']:\n",
    "        cutout_zipped = row['cutout_' + cutout]\n",
    "\n",
    "        cut_data = decode_img(cutout_zipped)\n",
    "\n",
    "        if normalize:\n",
    "            cut_data /= np.linalg.norm(cut_data)\n",
    "            cut_data -= np.nanmean(cut_data)\n",
    "        \n",
    "        cutout_dict[cutout] = np.nan_to_num(cut_data)\n",
    "\n",
    "    shape = np.shape(cutout_dict[cutout]) + (3,)         \n",
    "    triplet = np.zeros(shape)\n",
    "    triplet[:, :, 0] = cutout_dict['science']\n",
    "    triplet[:, :, 1] = cutout_dict['template']\n",
    "    triplet[:, :, 2] = cutout_dict['difference']\n",
    "    return triplet\n",
    "\n",
    "def rotate_triplet(triplet):\n",
    "    sci, ref, diff = triplet[:,:,0], triplet[:,:,1], triplet[:,:,2]\n",
    "    trip90 = np.stack((ndimage.rotate(sci, 90), ndimage.rotate(ref, 90), \n",
    "                       ndimage.rotate(diff, 90)), axis = -1)\n",
    "    trip180 = np.stack((ndimage.rotate(sci, 180), ndimage.rotate(ref, 180), \n",
    "                        ndimage.rotate(diff, 180)), axis = -1)\n",
    "    trip270 = np.stack((ndimage.rotate(sci, 270), ndimage.rotate(ref, 270), \n",
    "                        ndimage.rotate(diff, 270)), axis = -1)\n",
    "    return trip90, trip180, trip270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4657f9b7-94fd-4bca-a188-81a00994b9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.40258503e+00, -2.70922780e+00,  1.34585987e+01],\n",
       "        [-1.57603610e+00, -9.04600441e-01,  7.43493592e-01],\n",
       "        [-2.32718868e+01,  2.11288378e-01, -2.17775060e+01],\n",
       "        ...,\n",
       "        [ 3.19262892e-01,  8.80643654e+00, -4.75910853e+00],\n",
       "        [-7.62148798e-01,  1.61137268e-01,  4.80268557e-01],\n",
       "        [ 7.94740725e+00, -4.72291969e-02,  9.87890216e+00]],\n",
       "\n",
       "       [[-7.98119593e+00, -2.60333467e+00, -3.60900093e+00],\n",
       "        [ 1.54298000e+01, -1.60073006e+00,  1.86308245e+01],\n",
       "        [ 1.19404230e+01, -6.75896853e-02,  1.35212212e+01],\n",
       "        ...,\n",
       "        [ 7.07669592e+00,  9.75986099e+00,  1.18256759e+00],\n",
       "        [-1.53368979e+01,  2.56098151e+00, -1.75681883e+01],\n",
       "        [-1.61597958e+01,  5.13013065e-01, -1.53052568e+01]],\n",
       "\n",
       "       [[-1.39528875e+01, -4.86001641e-01, -1.23094679e+01],\n",
       "        [-3.60746837e+00, -3.18814844e-01, -9.87285110e-01],\n",
       "        [ 5.87591457e+00, -1.14580536e+00,  8.10771846e+00],\n",
       "        ...,\n",
       "        [-3.67479801e-01,  7.10276365e-01,  5.59003834e-01],\n",
       "        [-1.04564962e+01,  1.57081521e+00, -1.03118560e+01],\n",
       "        [-1.46012211e+01,  2.00713778e+00, -1.42208220e+01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.21261320e+01,  4.72312975e+00,  7.38903189e+00],\n",
       "        [ 4.00796356e+01,  3.11321115e+00,  3.88159206e+01],\n",
       "        [ 1.38502579e+02,  3.11159658e+00,  1.37128402e+02],\n",
       "        ...,\n",
       "        [ 1.20366497e+01,  5.86914015e+00,  6.51286243e+00],\n",
       "        [-5.83042431e+00,  4.34570599e+00, -1.26974845e+01],\n",
       "        [ 2.38181820e+01,  1.57511129e+01,  3.28146992e+00]],\n",
       "\n",
       "       [[ 3.14064865e+01,  4.43627453e+00,  2.85189693e+01],\n",
       "        [ 5.04986420e+01,  3.35717154e+00,  4.94794208e+01],\n",
       "        [ 4.97333221e+01,  3.77082753e+00,  4.89347625e+01],\n",
       "        ...,\n",
       "        [-2.73240547e+01,  6.08617353e+00, -3.14535061e+01],\n",
       "        [-6.90956831e+00,  4.56982994e+00, -1.09645702e+01],\n",
       "        [-1.37980576e+01,  5.16809893e+00, -2.04975768e+01]],\n",
       "\n",
       "       [[ 1.21282372e+02,  4.40387726e+00,  1.17025465e+02],\n",
       "        [ 7.76904602e+01,  3.81028175e+00,  7.50806539e+01],\n",
       "        [ 7.22967834e+01,  4.48290157e+00,  6.85981048e+01],\n",
       "        ...,\n",
       "        [ 4.42009735e+00,  6.53159571e+00, -6.23652496e-01],\n",
       "        [-6.82462549e+00,  6.34983921e+00, -1.11881309e+01],\n",
       "        [-2.07181334e+00,  3.83387613e+00, -5.13726876e+00]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_triplet(train_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7686e796-458e-4990-aa5b-b8f560f25353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real sources: 159, number of bogus sources: 400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of real sources: {len(REALS)}, number of bogus sources: {len(BOGUS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb25c375-8809-40e0-ad31-7a6b93cf5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking reals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/nrs6cfxd0d99x77tn7tlscsm0000gr/T/ipykernel_22616/4151298500.py:26: RuntimeWarning: Mean of empty slice\n",
      "  cut_data -= np.nanmean(cut_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking bogus\n"
     ]
    }
   ],
   "source": [
    "print('Stacking reals')\n",
    "cut_real = np.zeros(shape = (len(REALS)*4, 81, 81, 3))\n",
    "missing_reals = []\n",
    "for i, row in REALS.iterrows():\n",
    "    triplet = make_triplet(row, normalize = True)\n",
    "    t90, t180, t270 = rotate_triplet(triplet)\n",
    "    cut_real[4*i] = triplet    \n",
    "    cut_real[4*i + 1] = t90\n",
    "    cut_real[4*i + 2] = t180\n",
    "    cut_real[4*i + 3] = t270    \n",
    "    \n",
    "print('Stacking bogus')\n",
    "nb = len(BOGUS) #1500\n",
    "cut_bogus = np.zeros(shape = (len(BOGUS), 81, 81, 3))\n",
    "missing_bogus = []\n",
    "for i, row in BOGUS.iterrows():\n",
    "    triplet = make_triplet(row, normalize = True)\n",
    "    cut_bogus[i] = triplet    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddc6b12f-82b9-494e-872d-a99a9f04f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of real sources: 636\n",
      "Number of bogus sources: 400\n",
      "Total training samples: 1036\n"
     ]
    }
   ],
   "source": [
    "labels_real = np.ones(shape = len(cut_real))\n",
    "labels_bogus = np.zeros(shape = len(cut_bogus))\n",
    "labels = np.concatenate((labels_real, labels_bogus), axis = 0)\n",
    "cut = np.concatenate((cut_real, cut_bogus), axis = 0)\n",
    "\n",
    "print(f\"Number of real sources: {len(cut_real)}\")\n",
    "print(f\"Number of bogus sources: {len(cut_bogus)}\")\n",
    "print(f\"Total training samples: {len(cut)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b06bdf0-70be-4c34-b78a-54a919b61d73",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bfc9dd9-2636-446b-8bab-9022c623b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 0.1\n",
    "random_state = 95\n",
    "\n",
    "# make train and test masks:\n",
    "_, _, mask_train, mask_test = train_test_split(list(range(len(cut))), list(range(len(cut))), \n",
    "                                               test_size=test_split, random_state=random_state, shuffle = True)\n",
    "masks = {'training': mask_train, 'test': mask_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10973e1f-e4ea-4158-8b75-fdac74d8b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples (932, 81, 81, 3), testing samples (104, 81, 81, 3)\n"
     ]
    }
   ],
   "source": [
    "#create training and testing samples\n",
    "x_train, y_train = cut[mask_train], labels[mask_train]\n",
    "x_test, y_test = cut[mask_test], labels[mask_test]\n",
    "\n",
    "print(f\"Training samples {np.shape(x_train)}, testing samples {np.shape(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c2b31aa-375b-4033-86b9-9d5a4fad3b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtf\u001b[49m.keras.backend.clear_session()\n\u001b[32m      3\u001b[39m loss = \u001b[33m'\u001b[39m\u001b[33mbinary_crossentropy\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m optimizer = \u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# loss = 'binary_crossentropy'\n",
    "# optimizer = 'adam'\n",
    "# epochs = 500\n",
    "# patience = 50\n",
    "# validation_split = 0.1\n",
    "# class_weight = True\n",
    "# batch_size = 64\n",
    "\n",
    "# # halt training if no gain in validation accuracy over patience epochs\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "# data_augmentation = {'horizontal_flip': False,\n",
    "#                      'vertical_flip': False,\n",
    "#                      'rotation_range': 0,\n",
    "#                      'fill_mode': 'constant',\n",
    "#                      'cval': 1e-9}\n",
    "\n",
    "# #data generator\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=data_augmentation['horizontal_flip'],\n",
    "#                                                           vertical_flip=data_augmentation['vertical_flip'],\n",
    "#                                                           rotation_range=data_augmentation['rotation_range'],\n",
    "#                                                           fill_mode=data_augmentation['fill_mode'],\n",
    "#                                                           cval=data_augmentation['cval'],\n",
    "#                                                           validation_split=validation_split)\n",
    "\n",
    "# training_generator = datagen.flow(x_train, y_train, batch_size=batch_size, subset='training')\n",
    "# validation_generator = datagen.flow(x_train, y_train, batch_size=batch_size, subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c53380-48b4-408a-82b2-0303edb57e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "winterdrb",
   "language": "python",
   "name": "winterdrb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
